\chapter{Introduction}
An ever-growing number of algorithms are used to solve machine learning and data science tasks and the challenge of algorithm selection and configuration is subject to intensive research.  \citep{bischl-et-al,brazdil:p,calandra-et-al,collins-et-al2018,romero-et-al,vartak-et-al}. The \textbf{algorithm selection problem} characterises the challenge of finding an effective algorithm for a given dataset or task on an \textit{instance-by-instance basis} from a set of algorithms. This problem is motivated by the fact that different algorithm performs differently on different datasets. When put simply, while algorithm A performs poorly on a dataset X, when algorithm B performs well, choosing algorithm B over algorithm A for the given dataset X, without having to run both algorithm A and B to find out which of these is a better performer is the challenge of algorithm selection. There are \textit{various techniques} to find out which algorithm works better and few examples of these would be the winner-takes-all, heuristic approximation, feature analysis,  meta-learning and many more. In this research paper, the focus is on the Meta-Learning approach.

\textbf{Meta-learning} is one of the most promising techniques to warm starting the algorithm selection and configuration process \citep{hutter-et-al}. With meta-learning, a machine learning model is trained to predict how algorithms perform on a various tasks. The meta-learning model is built based on the past performance of algorithms on a large number of tasks or datasets, which are described through meta-features. It does this by learning the relationships between the different meta-features of data, and performance of the algorithm on the data. For unseen tasks, the best performing algorithms can be predicted through the meta-learner (and subsequently be optimized using techniques such as Bayesian Hyper-Parameter Optimization). These meta-features are also known as the characterisation measures and there are no standard set of meta-features which can be used to describe the dataset accurately, but there are several approaches to characterise the dataset as described in \citep{feurer:m}, \citep{meta-features-1} and \citep{meta-features-2}. These meta-features are used to describe and characterise the dataset, which is then used to provide an assessment of algorithm's performance.

\section{AutoML}
Automated Machine Learning or AutoML, is the process of automating the time consuming task of Algorithm Selection and Optimizing the Hyper-Parameters. The task of Algorithm Selection is usually done on an instance by instance basis. There are many tools which offer this feature like Auto-Sklearn, OpenML, etc., but these tools select the best performing algorithm by predicting the performance of each algorithm for a given task by running various algorithms on a subset of the entire dataset and then choosing the best performing algorithm on that subset for the task. Though this is just a over-simplified explanation of what the libraries do internally, this is a time consuming task, and in-turn electricity and computational power before it could suggest the best performing algorithm for the task. Even if the task is to find the best algorithm for a previously seen dataset the entire process is repeated.

The same can be said for Hyper-Parameter Optimisation / Tuning as well, this is a process of selecting a set of optimal hyper-parameters for an algorithm. A hyper-parameter is a parameter whose value is used control how a machine learning algorithm solves a given problem. Hyper-Parameter Optimisation is also a task that takes a great deal of time, electricity and computational power, before an+ optimal set of hyper-parameter can be chosen for an algorithm-dataset pair. Hyper-Parameters are selected by building models based on a combination of parameters specified and evaluating the models to find the best set of parameters for which the selected algorithm performs the best.

\section{Motivation}
A challenge in algorithm selection and configuration is the (non) availability of data in some disciplines to build the meta-learning model, which is due to the workflow of machine learning, data science or other projects. Typically, software libraries – be it machine learning libraries like Auto-sklearn \citep{feurer:m}, Auto-Weka \citep{kotthoff:l} or ML-Plan \citep{mohr:f}, recommender system libraries like LibRec-Auto \citep{mansoury:m}, Auto-Suprise \citep{rohan-joeran} are used in isolation, either locally or in the cloud. By “in isolation” meaning, that the information regarding how algorithms perform on a particular dataset, is neither published nor shared. Consequently, computationally expensive algorithm selection and hyper parameter optimization is performed by each machine learning engineer over and over again for the same datasets, which is a huge waste of Time, Electricity, Computational Power and Money.

\section{Research Problem}
The resources such as time spent for the task of algorithm selection and hyper-parameter optimisation is justifiable for the first time search for the best algorithm is done. On the other hand, with the increasing popularity and use of Machine Learning for various tasks, time, electricity and computational power spent for finding the best algorithm for a given a dataset, along with it's optimal hyper-parameters becomes a repetitive task. 

Considering a small and simple dataset like the Breast Cancer Dataset with 569 records and 30 features spending 94.24 minutes to find the best performing algorithm is a very long time the developer has to wait before proceeding with further computational tasks. This also means an average of about 125.65W of power is being consumed by the computer (check Appendix \ref{machine-details} for power consumption on the tested machine). For a large dataset like the Skin Segmentation dataset with 245057 records and 4 features, it takes approximately 869.74 minutes to find the best performing algorithm and this task consumes approximately 1159.65W of energy. Even if this task is repeated a couple of hundred times, the electricity wasted performing the same task repeatedly is huge!

The problem here is the workflow of machine learning and data science libraries, these libraries usually work in isolation and the information regarding how an algorithm performs on a particular dataset is neither publisher nor shared. This leads to computationally expensive and time consuming task like algorithm selection and hyper-parameter optimisation being performed repeatedly for the same dataset.

\section{Federated Meta Learning}
Beel proposed “Federated Meta-Learning” \citep{fml}, a concept that allows everyone to benefit from the data that is generated through software libraries including standard machine learning and data science libraries as well as the auto* tools. 

Federated Meta Learning focuses on learning the algorithm performance measures for arbitrary tasks across devices. He envisioned federated meta learning as an ecosystem where the raw data is kept on the original  devices and the meta data, algorithm names, and performance metrics of the algorithm on the tasks would be stored on a central FML server (though a peer-to-peer architecture might also be possible). Using this historic performance data, a model is trained and is used to predict the best performing algorithm along with its hyper-parameters for a previously seen or unseen task.


\section{Research Goal}
The goal of this research is to facilitate the algorithm selection and configuration process. This can be done by making it faster with the use of historic performance data, produced on various devices and by various machine learning algorithms and libraries. This data can be used to improve the performance of algorithm selection, thus saving time, electricity, computational power and money which will otherwise be required for finding the best algorithm and its optimal hyper-parameters for the task.

To improve the algorithm selection and configuration process, a prototype of Federated Meta Learning named \textbf{FMLearn} is presented. The potential benefits of this research include the development of FMLearn a tool that applies the concepts of Federated Meta-Learning to suggest users with the best algorithm along with it's hyper-parameters for a previously seen or unseen tasks.

%% \subsection{FMLearn}
The goal is to develop an application, \textbf{FMLearn} as a simple proof of concept for Federated Meta-Learning and it should allow everyone to benefit from the data that is generated through machine learning and data science libraries. When put simply, the input to FMLearn is a dataset and the output is a recommendation for the potentially best performing algorithm(s) and it's hyper-parameters to solve that task.

%% multi-time comment using if block
\if false
FMLearn consists of a server\footnote{\url{https://github.com/mukeshmk/fm-learn}} and a client, in our case a modified scikit-learn\footnote{\url{https://github.com/mukeshmk/scikit-learn}}, but it could be any machine learning library.
\fi

FMLearn also provides an additional benefit by also acting as a publicly available knowledge base or directory of algorithms-data performance measures with an ability to improve and/or add data to this knowledge base. With this it also provides an ability to use this knowledge via the API's provided by the FMLearn application for other similar tasks. Another important benefit is the ability for any Machine Learning or Data Science tool to use FMLearn irrespective of the programming language used to build them to get a recommendation for the potentially best performing algorithm(s) and it's hyper-parameters to solve it's task without being constrained by the need to use the client that is supported by this paper (modified scikit-learn).


\section{Contributions}
This research utilises the novel concept of Federated Meta-Learning in it's core, applies it's principle and proposes the first prototype of the application FMLearn. The application will ultimately facilitate the algorithm selection and configuration process in finding the best performing algorithm for a given task. Apart from this, the research also makes the following contributions to the community:

\begin{itemize}
    \item FMLearn also acts a knowledge base or directory of algorithms-data performance measures.
    \item Provides a publicly available API server to facilitate access of the performance measures and also the algorithm selection and configuration process.
    \item Provides a client implementation in python to access the API's provided by the server.
    \item This research also acts a proof of concept for Federated Meta-Learning and how it helps in saving time and hence electricity, computational power and money for the user.
\end{itemize}

This research was also published at the \textit{7th ICML Workshop on Automated Machine Learning (AutoML)}, under the name \textit{Federated Meta-Learning: Democratizing Algorithm Selection Across Disciplines and Software Libraries}.

%% multi-time comment using if block
\if false

\subsection{Research Tasks}
The various tasks that are to be performed in this research, which will ultimately facilitate the algorithm selection and configuration process in finding the best algorithm for the task are:

\begin{itemize}
    \item Create a knowledge base or directory of algorithms-data performance measures.
    \item Develop a client-server architecture based system to facilitate the algorithm selection and configuration process.
    \item Find the best algorithm to predict which algorithm is the best suited for the task.
    \item Analyse how Federated Meta-Learning helps in saving time and hence electricity, computational power and money for the user by finding the best algorithm for the task.
\end{itemize}

\section{Research Scope}
This research / project will solely focus on whether any benefits can be gained by using the concept of Federated Meta-Learning via the application FMLearn. This research / project will mainly focus on the performance improvements gain by using FMLearn and also the time, electricity, computational power and money saved by the developers / users who use this application to get a recommendation for the potentially best performing algorithm(s) and it's hyper-parameters to solve a given task. Potential security and privacy concerns will be shortly discussed to give future researchers a scope for research and improvement to be applied in this project. However, this will not be the primary concern of this paper.

\fi